[{"body":"","link":"http://localhost:1313/","section":"","tags":null,"title":""},{"body":" Dummy line that gets swallowed by the above image... Software developer and systems administrator looking to network. I have a passion for Linux and open source, and I enjoy contributing to projects in my free time. Follow me on the Gits:\nhttps://github.com/PC-Admin\nhttps://gitlab.com/PC-Admin ","link":"http://localhost:1313/about/","section":"","tags":null,"title":"About Me"},{"body":"","link":"http://localhost:1313/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"http://localhost:1313/tags/ceph/","section":"tags","tags":null,"title":"Ceph"},{"body":"Geo-Redundant Storage Made Easy Georedundant storage is hard, companies pay a lot of money for serious guarantees that their data will not only be replicated across multiple locations but that it'll be available to them when they need it. This is a problem that's difficult to solve, and it's why companies like AWS charge so much for their multi-region storage solutions.\nCeph is an open source distributed storage system that is designed to be highly available and fault-tolerant. It spreads data across multiple 'nodes' (hosts) and disks, so that if one node or disk fails, that data is still available. Configuring Ceph to make data redundant across one site can be a challenge, but this is made even more difficult when you want to replicate data across multiple sites while ensuring reliability. A multi-site Ceph setup requires running multiple Ceph clusters in an active-active or active-passive configuration, it is significantly more complex than a single-site setup.\nThis is where Ceph's \u0026quot;stretch mode\u0026quot; feature comes in. If you only want to distribute data across 2 sites and you want to avoid the complexity of a multi-site setup, stretch mode is the happy middle ground you're looking for.\nLimits of a Ceph Cluster CRUSH picks OSDs at random to write data to, it doesn't guarantee that data will be replicated across multiple sites. This is a problem that is difficult to solve with standard CRUSH rules, as the documentation states:\nhttps://docs.ceph.com/en/latest/rados/operations/stretch-mode/#stretch-cluster-issues\n\u0026quot;For example, in a scenario in which there are two data centers named Data Center A and Data Center B, and the CRUSH rule targets three replicas and places a replica in each data center with a min_size of 2, the PG might go active with two replicas in Data Center A and zero replicas in Data Center B. In a situation of this kind, the loss of Data Center A means that the data is lost and Ceph will not be able to operate on it. This situation is surprisingly difficult to avoid using only standard CRUSH rules.\u0026quot;\nHow Stretch Mode Works The standard configuration for stretch mode is 2 datacenters with a roughly equal amount of OSDs in each. Each site holds 2 copies of the data with a total replication size of 4. New writes are written once to both sites before they are acknowledged by Ceph. A third site is used as a tiebreaker, which doesn't contain any OSDs. In the event that the network between the 2 sites is cut, the tiebreaker site will be used to determine which site is the primary site. When the network between the 2 sites is restored, the secondary site will be brought back online and the data will be resynced.\nIn the event that one datacenter is lost, the cluster will enter a special \u0026quot;degraded stretch mode\u0026quot; described here:\nhttps://docs.ceph.com/en/latest/rados/operations/stretch-mode/#entering-stretch-mode\n\u0026quot;If all OSDs and monitors in one of the data centers become inaccessible at once, the surviving data center enters a “degraded stretch mode”. A warning will be issued, the min_size will be reduced to 1, and the cluster will be allowed to go active with the data in the single remaining site. The pool size does not change, so warnings will be generated that report that the pools are too small -- but a special stretch mode flag will prevent the OSDs from creating extra copies in the remaining data center. This means that the data center will keep only two copies, just as before.\u0026quot;\nThe Benefits of Stretch Mode Cross-site data redundancy and high availability. It's easier to set up than a multi-site configuration; you don't need to run multiple Ceph clusters or configure zones, realms and endpoints. It's cheaper than a multi-site setup, as a multi-site setup requires running a duplicate amount of hardware in both locations to host 3 copies in each. While a stretch mode cluster only keeps 2 copies in each location. The Limitations of Stretch Mode It's only suitable for 2 sites, if you want to replicate data across more than 2 sites you'll need to create a multi-site setup. It's only suitable if your OSD devices are SSDs, as the latency of HDDs is too high to make this work. Erasure coded pools cannot be used or created while using stretch mode. Writes are slow, this is because Ceph needs to write an object to both sites before it can acknowledge the write. To find out more about the limitations of stretch mode see the officical documentation here:\nhttps://docs.ceph.com/en/latest/rados/operations/stretch-mode/#limitations-of-stretch-mode\nInstall Guide for Ceph's Stretch Mode I've published a guide for creating a small testing cluster with stretch mode enabled, it can be used to help you examine if a stretch mode is appropriate for your own use case:\nhttps://github.com/PC-Admin/cephfs-stress-test/blob/main/stretch_mode_setup.md\nTesting Site Failure I also tested an abrupt site failure on this cluster, I did this by forcibly shutting off the VMs in datacenter a1. The filesystem became unresponsive for 33 seconds, and then it was back to normal.\nI then tested a sudden isolation of the network between the 2 sites, where both could talk to the tiebreaker node but not to each other. I did this by quickly erecting firewall rules on the hosts in a1 and severing any existing connections. The filesystem became unresponsive for 47 seconds before returning to normal.\nI've published the steps and results of these tests here:\nhttps://github.com/PC-Admin/cephfs-stress-test/blob/main/stretch_mode_setup_tests.md\nStretch mode seems to be very resilient and allows clients to keep accessing their data pretty quickly in the event of a site failure.\nConclusion Stretch mode is a great feature that allows you to replicate data across multiple sites without having to set up a full-blown multi-site cluster with multiple clusters and a lot of added complexity. It's cheaper than a multi-site setup, as a multi-site setup requires running a duplicate amount of hardware in both locations to host 3 copies in each. While a stretch mode cluster only keeps 2 copies in each location.\nIf you need assistance with your Ceph setup, or your interested in upgrading to a stretch mode cluster, please contact me for a free consultation.\n","link":"http://localhost:1313/post/georedundant-storage-made-easy/","section":"post","tags":["ceph","georedundancy","distributed storage"],"title":"Ceph's Stretch Mode: Georedundant Storage Made Easy!"},{"body":"","link":"http://localhost:1313/tags/distributed-storage/","section":"tags","tags":null,"title":"Distributed Storage"},{"body":"","link":"http://localhost:1313/categories/distributed-storage/","section":"categories","tags":null,"title":"Distributed Storage"},{"body":"","link":"http://localhost:1313/tags/georedundancy/","section":"tags","tags":null,"title":"Georedundancy"},{"body":"","link":"http://localhost:1313/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"http://localhost:1313/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"http://localhost:1313/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"http://localhost:1313/tags/2fa/","section":"tags","tags":null,"title":"2fa"},{"body":"","link":"http://localhost:1313/tags/cybersecurity/","section":"tags","tags":null,"title":"Cybersecurity"},{"body":"","link":"http://localhost:1313/categories/cybersecurity/","section":"categories","tags":null,"title":"Cybersecurity"},{"body":"","link":"http://localhost:1313/tags/security-keys/","section":"tags","tags":null,"title":"Security Keys"},{"body":"As my own security requirements have risen, I've needed a way to bolster the security of my accounts and other keys. One of the most effective ways to achieve this is by using a Universal 2nd Factor (U2F) security key. In this article, we'll explore the advantages of U2F security keys over traditional OTP and text 2FA, along with my personal experience of securing my online accounts and computer systems using Solokey v1 U2F devices.\nWhat is a U2F Security Key? A U2F security key is a physical device that is used for two-factor authentication (2FA) and multi-factor authentication (MFA). It works by requiring users to insert the key into a USB port and touch the button on the key to authenticate themselves. U2F is an open standard developed by the FIDO Alliance, which is a non-profit organization dedicated to reducing the reliance on passwords for online authentication.\nCompared to traditional OTP and text 2FA, U2F security keys provide a higher level of security. OTPs and text 2FA are vulnerable to phishing attacks, where attackers can trick users into providing their login credentials. With U2F, the user's private key never leaves the device, making it virtually impossible for attackers to gain access to their accounts unless the physical security of the key is compromised.\nSecuring Online Accounts with U2F After purchasing a couple of Solokey v1 devices. The first step was to set up U2F authentication for my Google, Facebook, PayPal, and email accounts. This process involved registering the SoloKey as a security key on each account and then inserting the key and pressing the button to authenticate myself. With each account I'm also making sure to register my backup Solokey as well, this allows me to not loose access if one of my Solokeys is lost.\nUsing U2F authentication was an easy and quick experience. Instead of having to type in a code or wait for a text message, all I had to do was insert the key and press the button. It is a relief to know that my more critical online accounts are now much more secure than before.\nSolokey Setup (Linux only) First install prerequisite packages:\n1$ sudo apt install git llvm clang libclang-dev gcc-arm-none-eabi gdb-arm-none-eabi libc6-dev-i386 Then connect your Solokeys and add a pin then update them one by one:\n1$ solo key update 2Wrote temporary copy of firmware-4.1.5.json to /tmp/tmp3qnvhtlp.json 3sha256sums coincide: f36bb365bfddf75004f28af392ae1439192ca0ed821ef49429675a00d05087a 4using signature version \u0026gt;2.5.3 5erasing firmware... 6updated firmware 100% 7time: 9.25 s 8bootloader is verifying signature... 9...pass! 10 11Congratulations, your key was updated to the latest firmware version: 4.1.5 12 13pcadmin@workstation:~/$ solo key set-pin 14Please enter new pin: 15Please confirm new pin: 16Done. Please use new pin to verify key Securing Server Access and GitHub/GitLab Commits with U2F As a software developer, I also needed to up the security of my server access and GitHub/GitLab commits. With U2F, I could achieve this effortlessly by using the key with SSH. I set up my SSH client to require U2F authentication and then registered the SoloKey as a security key on my server and GitHub/GitLab accounts. Whenever I needed to log in to my server or make a commit, I would insert the key and press the button to authenticate myself.\nUsing U2F authentication for server access and GitHub/GitLab commits added an extra layer of security to my development workflow. It prevented anyone who might have stolen my SSH key or GitHub/GitLab credentials from accessing my accounts.\nGenerating the SSH key requires the -O resident flag to ensure the credential is discoverable:\n1$ ssh-keygen -t ed25519-sk -O resident -f ~/.ssh/michael2023-red -C \u0026#34;Michael Collins 2023 - Red Key - michael@perthchat.org\u0026#34; If done properly you can check the new openssh credential on your solokey:\n1$ solo key credential ls 2PIN: 3Relying Party Username Credential ID 4----------------------------------------------------- 5ssh: openssh qPxzGbiMX5uvJOSWnDkvMGQgqhkiNIKvK5dI4XogHQG8ZuMGEOihYhFZYP4ewiPmUpyfS26AIA3LXlwyHIrx4rG/LwEAAA== Securing My GNOME Desktop with U2F Using U2F authentication for my GNOME desktop provides an additional layer of security that goes beyond a traditional password. Even if someone were to obtain my password, they would still need physical access to my U2F security key to gain access to my system. Your U2F device can also be used to replace a traditional password for extra ease-of-use, although this is not as secure.\nAfter installing the libpam-u2f package, I registered the SoloKey as a security key on my system and then configured PAM to require U2F authentication for login and sudo access. Now, whenever I log in to my GNOME desktop or run a sudo command, I have to insert the key and press the button to authenticate myself.\nTo configure, just install libpam-u2f and add a pam_u2f.so entry with either 'required' or 'sufficient' infront of the @include common-auth line:\n1$ sudo apt install libpam-u2f -y 2$ cat /etc/pam.d/gdm-password 3#%PAM-1.0 4auth requisite pam_nologin.so 5auth\trequired\tpam_succeed_if.so user != root quiet_success 6auth required pam_u2f.so 7@include common-auth 8auth optional pam_gnome_keyring.so 9... The 'required' setting here adds security by requiring the U2F key as a 2FA to your GNOME keyring password. If you're device is in a safe location you could set it to 'sufficient' for convenience, this means that only inserting then pressing the button on your U2F key is required to login to the device.\nYou can also add U2F security to your sudo via these PAM files, here we can see I've chosen convenience over security and have set the U2F key as 'sufficient', so I no longer need to bother typing my sudo password:\n1$ cat /etc/pam.d/sudo 2#%PAM-1.0 3 4# Set up user limits from /etc/security/limits.conf. 5session required pam_limits.so 6 7auth sufficient pam_u2f.so 8@include common-auth 9@include common-account 10@include common-session-noninteractive Conclusion Overall I'm pretty happy with my Solokey v1 devices, they are affordable and more importantly reputable due to having both open-source hardware and software. I'm also looking forward to the release of the new SoloKey v2, it apparently has NFC, reversible USB-A and a more secure and open source friendly microcontroller:\nThere are a lot of secure microcontrollers out there, but very few that can be used for open source projects. Our LPC55S69 is one of the latest by NXP and doesn't require a thicket of NDAs. It supports secure boot, TrustZone, asymmetric crypto acceleration (ECC, RSA), PUF, and filesystem encryption. Together, these features enable a high level of security, secure firmware updates, speedy operations, and passive NFC operation.\nI would recommend investing in a U2F security key and using it to secure your online accounts, server access, and computer systems. It may require a bit of initial setup, but it is well worth it if your organisation requires the extra security or ease-of-use that U2F can provide.\n","link":"http://localhost:1313/post/solokey-u2f/","section":"post","tags":["security keys","cybersecurity","2fa"],"title":"Solokey U2F"},{"body":"","link":"http://localhost:1313/tags/moosefs/","section":"tags","tags":null,"title":"Moosefs"},{"body":"","link":"http://localhost:1313/tags/raid/","section":"tags","tags":null,"title":"Raid"},{"body":"Your Network Storage When it comes to data storage, many organizations turn to RAID as a popular option. RAID technology allows multiple physical hard drives to be combined into a single logical unit, providing improved performance and data redundancy. However, despite its popularity, RAID has its drawbacks and can be dangerous, leading to more risk of data loss and downtime when compared to distributed storage systems like MooseFS.\nPoints of Failure One of the primary issues with RAID is the risk of a single point of failure. In most RAID configurations, data is spread across multiple drives, and if one of those drives fails, the data may become inaccessible. Even RAID configurations that offer redundancy through mirroring or parity can be problematic. Mirroring requires twice the number of physical drives and can be costly. Meanwhile, parity-based RAID can suffer from the \u0026quot;write hole\u0026quot; problem, where data corruption can occur due to power outages or unexpected shutdowns.\nMooseFS, on the other hand, offers multiple host redundancy. This feature ensures that there is no single point of failure, meaning that even if one or more of the physical drives fails, the data will still be accessible from another node. This redundancy prevents downtime and data loss, providing peace of mind for organizations who store important data or have critical applications that rely on their storage.\nScalability Another issue with RAID is scalability. While RAID can provide improved performance and redundancy for small to medium-sized data sets, it can become less effective as data sets grow in size. As the number of drives in a RAID array increases, the risk of a drive failure also increases. This can lead to longer rebuild times and increased downtime.\nMooseFS, on the other hand, is designed to be highly scalable and can handle very large data sets with ease. MooseFS uses a distributed file system architecture, allowing data to be spread in chunks across many 'bricks' (disks) on multiple hosts. MooseFS can handle scaling up to petabytes of data, combining the network bandwidth of many hosts for higher speed applications, it offers higher levels of parallelism and scalability then RAID can provide. For organizations that need to store and manage larger amounts of data, MooseFS is an ideal choice.\nData Integrity Data integrity is also a critical issue in data storage. RAID provides some level of data redundancy, but it does not guarantee data integrity. If data corruption occurs on one drive, the corrupted data may be mirrored or parity-checked onto the other drives, resulting in data loss or corruption across the entire array. RAID uses error correction code algorithms with up to 9 parity sums to ensure data redundancy. However, this approach can be inefficient as it saves less RAW space compared to an ordinary data duplication approach. In contrast, MooseFS ensures data redundancy by using distributed data replication across multiple storage nodes. This means that if one node fails, data can be retrieved from other nodes, eliminating the risk of data loss or downtime.\nMooseFS offers the ability to adjust the redundancy of data on a per folder basis. So less important data can be made less redundant to save disk space, while more important data can be set to maximise redundancy and safety. MooseFS also offers efficient atomic snapshotting, ensuring that data integrity is maintained at all times. With atomic snapshotting, MooseFS takes a snapshot of the data at a specific point in time, ensuring that any changes made to the data are not reflected until the snapshot is complete. This approach ensures that even if there is data corruption, the snapshot can be used to restore the data to a known good state.\nAnother advantageous feature of MooseFS is its virtual, global space for deleted objects, which can be configured for each file and directory. This feature makes it easy to recover accidentally deleted data, a common issue in RAID data storage.\nPerformance RAID was initially developed in the 1980s as a means of improving data storage reliability by distributing data across multiple disks. However, as storage technology has advanced, RAID has become less effective and more prone to failure. One of the biggest issues with RAID is that it relies on a single central server or network connection, which can become a bottleneck for I/O operations. This can lead to slower data access times and increased downtime for organizations.\nMooseFS Pro offers high availability master nodes to ensure that the file system's central management component is always accessible and reliable. The high availability feature provides redundant master nodes, so that if one master node fails or becomes unreachable, another node can seamlessly take over its functions. This helps to prevent data loss and downtime, ensuring that the system remains available to users even in the event of hardware failures or other issues.\nMooseFS Pro's EC (Erasure Coding) feature can save disk space by reducing the amount of redundancy needed to protect data. For example, with a traditional replication-based storage system, you may need to store three copies of your data to ensure data integrity. With EC, you can achieve the same level of data protection with fewer copies and less redundancy. For a 200TB cluster, using replication-based storage, you would need 600TB of raw storage capacity to store three copies of your data. However, with EC, you could achieve the same level of data protection with only 400TB of raw storage capacity, assuming a 2+2 erasure coding scheme. This represents a significant savings in disk space, without sacrificing data integrity.\nHardware Independance Traditional RAID systems typically require that all disks be uniform in type and size, which can restrict flexibility and drive up costs.\nIn contrast, MooseFS offers the advantage of supporting both all-flash and hybrid storage setups, allowing for a mix of different manufacturers' disks and servers within a single clustered storage system. This means that the system can accommodate both older and newer technology, as well as both consumer-grade and enterprise-grade hardware. Furthermore, MooseFS allows hosts with different operating systems to be combined, enabling seamless integration with various environments. With MooseFS, users can easily and incrementally scale up their storage capacity as their needs and budget change, or alternatively downsize and repurpose hardware as needed, providing a highly flexible and cost-effective storage solution.\nHere we see my own MooseFS cluster that combines a consumer grade amd64 computer with 3 low power arm64 Helios64 devices:\nSummary In conclusion, while RAID has been a popular choice for many years, it is not without its drawbacks. RAID can be dangerous and can cause more downtime when compared to MooseFS Community Edition or Pro. MooseFS offers multiple host redundancy ensuring there's no single point of failure, efficient atomic snapshotting, high reliability, parallelism, and scalability. These features make MooseFS an excellent choice for organizations looking to store and manage large amounts of data while minimizing the risk of downtime and data loss. Overall, these features make MooseFS a superior option for organizations looking for a reliable and efficient storage system.\nAre you considering deploying MooseFS as a network storage solution for your company? If so, I invite you to schedule a complimentary consultation with me to explore the ways in which MooseFS could enhance your business.\n","link":"http://localhost:1313/post/raid-vs-moosefs/","section":"post","tags":["raid","moosefs","distributed storage"],"title":"RAID vs MooseFS"},{"body":"","link":"http://localhost:1313/archives/","section":"","tags":null,"title":""},{"body":"","link":"http://localhost:1313/series/","section":"series","tags":null,"title":"Series"}]